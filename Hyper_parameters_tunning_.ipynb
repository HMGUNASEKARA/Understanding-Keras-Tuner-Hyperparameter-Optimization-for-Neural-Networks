{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN4gyPM432dyV9KLtB8i+SV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HMGUNASEKARA/Understanding-Keras-Tuner-Hyperparameter-Optimization-for-Neural-Networks/blob/main/Hyper_parameters_tunning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the simple feed Forward model"
      ],
      "metadata": {
        "id": "ORrl7pzp6sKb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "51awe0Cy61gK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2igH2tXS6bTf",
        "outputId": "a1f14d01-dca2-4e06-9d14-93da30205b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 0.2250 - val_loss: 0.1087\n",
            "Epoch 2/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1043 - val_loss: 0.1036\n",
            "Epoch 3/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0924 - val_loss: 0.1025\n",
            "Epoch 4/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0854 - val_loss: 0.1013\n",
            "Epoch 5/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0901 - val_loss: 0.1023\n",
            "Epoch 6/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0902 - val_loss: 0.1022\n",
            "Epoch 7/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0832 - val_loss: 0.1015\n",
            "Epoch 8/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0817 - val_loss: 0.1039\n",
            "Epoch 9/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0858 - val_loss: 0.1015\n",
            "Epoch 10/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0837 - val_loss: 0.1013\n",
            "Epoch 11/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0861 - val_loss: 0.1030\n",
            "Epoch 12/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0815 - val_loss: 0.1058\n",
            "Epoch 13/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0819 - val_loss: 0.1041\n",
            "Epoch 14/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0820 - val_loss: 0.1033\n",
            "Epoch 15/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0809 - val_loss: 0.1047\n",
            "Epoch 16/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0802 - val_loss: 0.1031\n",
            "Epoch 17/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0817 - val_loss: 0.1075\n",
            "Epoch 18/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0789 - val_loss: 0.1033\n",
            "Epoch 19/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0798 - val_loss: 0.1047\n",
            "Epoch 20/20\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0827 - val_loss: 0.1063\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential  # Methaninn pahala ewa input karapu nisa\n",
        "from tensorflow.keras.layers import Dense       # Model eka build karanakota\n",
        "from tensorflow.keras.optimizers import Adam    # tf.keras........ kiya kiya digata type karanna one na\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Example dummy data (replace with your stock dataset)\n",
        "X = np.random.rand(1000, 10)  # 10 features\n",
        "y = np.random.rand(1000, 1)\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Build simple ANN model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "history = model.fit(X_scaled, y, epochs=20, batch_size=32, validation_split=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vopMnT4V7d3o",
        "outputId": "1d0d81d2-56ef-42a0-f226-f85f8067f706"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.12/dist-packages (1.4.8)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (2.32.4)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.76.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (5.29.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio->keras-tuner) (4.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt   # Import keras tuner library and name as kt\n",
        "from tensorflow.keras.layers import Dense  # Import dence layers\n",
        "from tensorflow.keras.models import Sequential # Imports the Sequential model class\n",
        "from tensorflow.keras.optimizers import Adam # Import adam optimizer\n",
        "\n",
        "def build_ann(hp):  #  Define the Function , This is expect the Keras tuner,\n",
        "# tuner  called this many time, Each time with different hyperparameter choises\n",
        "# hp = seach space\n",
        "    model = Sequential() # Create empty sqential model with object name called model\n",
        "\n",
        "    # Here tune the number o hidden layers decide how many\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "        model.add(Dense(  # Define the layers\n",
        "            units=hp.Int(f\"units_{i}\", min_value=32, max_value=256, step=32), # Define the number of units in each layer,32 to 256\n",
        "            activation=hp.Choice(f\"activation_{i}\", ['relu', 'tanh']) # define the activation function in each layer here used Tanh and relu.\n",
        "        ))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1)) # Out put layer\n",
        "\n",
        "    # Learning rate tuning\n",
        "    lr = hp.Choice(\"learning_rate\", [1e-4, 1e-3, 1e-2]) # Define 3 learnig rate\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr), # Here used adamoptimizer with 3 learnig Rates\n",
        "        loss=\"mse\"\n",
        "    )\n",
        "\n",
        "    return model # After creating the model accoding to the specific senario return themodel\n"
      ],
      "metadata": {
        "id": "MlIJtg-P8atQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the hyperband to tune the model\n",
        "tuner = kt.Hyperband( # Create hyperband model object\n",
        "    build_ann, # Function that used to build the model\n",
        "    objective='val_loss',  # Tell tuner to choose the best model base on the validation loss\n",
        "    max_epochs=30,  # Hyperband will train each model upto 30 epochs ba model stop early\n",
        "    factor=3, # This control how aggressively reduce the the number of models\n",
        "    directory='ann_tune_dir', # Give directry to the save the tunning ,Then next time this is not do this again\n",
        "    project_name='ann_tuning'  # Simply look the save location\n",
        ")\n",
        "\n",
        "# Hyper band is efficent hyperparameter optimization\n",
        "# Train many model with fewer epochs\n",
        "# Stop bad models early\n",
        "# Trains promising models fro langer\n",
        "# Find the best model fast\n",
        "\n",
        "# factor = 3  what happen here\n",
        "\n",
        "# In the first stage hyperband train different model in lower number of epochs then remaing only\n",
        "# 1/3 amout of for next stage, then the next stage remaing.. Like wise finally got the best model\n",
        ""
      ],
      "metadata": {
        "id": "qdVrPO438fdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b388c58b-9590-4e98-c3d7-9b4ee287e772"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from ann_tune_dir/ann_tuning/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X_scaled, y, epochs=30, validation_split=0.1)\n",
        "# By using this code train the model in each model architectre and trail.\n",
        "# Like this\n"
      ],
      "metadata": {
        "id": "eImSBsZz8i8D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "# After tuning we have different combination of parameres, Identify the best one with lowest validtion test\n",
        "# Tell we need only one best posible combination\n",
        "\n",
        "print(\"Best number of layers:\", best_hps.get(\"num_layers\")) # extract and print best number of layers\n",
        "for i in range(best_hps.get(\"num_layers\")): # loop through the each layer and print\n",
        "    print(f\"Layer {i} — Units:\", best_hps.get(f\"units_{i}\")) # Number of layer and unit amount\n",
        "    print(f\"Layer {i} — Activation:\", best_hps.get(f\"activation_{i}\")) # Number of layers and actvation\n",
        "\n",
        "print(\"Best Learning Rate:\", best_hps.get(\"learning_rate\")) # Best learning rate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B7Qfdi3-K6y",
        "outputId": "e70ff954-97b8-487f-bbd1-0710e012cfda"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of layers: 1\n",
            "Layer 0 — Units: 32\n",
            "Layer 0 — Activation: tanh\n",
            "Best Learning Rate: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tuner.hypermodel.build(best_hps) # Tell the tuner to build the finalbest model\n",
        "\n",
        "history = best_model.fit( # and tell train thebest model\n",
        "    X_scaled, y,\n",
        "    epochs=50,\n",
        "    validation_split=0.1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzuiATfP-AuO",
        "outputId": "3c389c6f-8b25-4dcf-d5b3-f35aacb90502"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.3798 - val_loss: 0.1081\n",
            "Epoch 2/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0923 - val_loss: 0.0988\n",
            "Epoch 3/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0849 - val_loss: 0.0987\n",
            "Epoch 4/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0861 - val_loss: 0.0990\n",
            "Epoch 5/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0862 - val_loss: 0.0982\n",
            "Epoch 6/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0850 - val_loss: 0.1159\n",
            "Epoch 7/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0903 - val_loss: 0.1019\n",
            "Epoch 8/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0822 - val_loss: 0.1036\n",
            "Epoch 9/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0837 - val_loss: 0.1034\n",
            "Epoch 10/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0883 - val_loss: 0.0992\n",
            "Epoch 11/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0834 - val_loss: 0.0987\n",
            "Epoch 12/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0804 - val_loss: 0.0980\n",
            "Epoch 13/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0817 - val_loss: 0.1135\n",
            "Epoch 14/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0893 - val_loss: 0.0998\n",
            "Epoch 15/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0870 - val_loss: 0.1084\n",
            "Epoch 16/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0835 - val_loss: 0.0980\n",
            "Epoch 17/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0940 - val_loss: 0.1078\n",
            "Epoch 18/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0870 - val_loss: 0.1005\n",
            "Epoch 19/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0924 - val_loss: 0.1066\n",
            "Epoch 20/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0898 - val_loss: 0.0986\n",
            "Epoch 21/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0855 - val_loss: 0.1013\n",
            "Epoch 22/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0865 - val_loss: 0.1028\n",
            "Epoch 23/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0841 - val_loss: 0.0998\n",
            "Epoch 24/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0832 - val_loss: 0.0991\n",
            "Epoch 25/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0810 - val_loss: 0.1013\n",
            "Epoch 26/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0960 - val_loss: 0.1039\n",
            "Epoch 27/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0787 - val_loss: 0.0996\n",
            "Epoch 28/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0830 - val_loss: 0.1023\n",
            "Epoch 29/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0883 - val_loss: 0.0985\n",
            "Epoch 30/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0817 - val_loss: 0.1024\n",
            "Epoch 31/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0778 - val_loss: 0.1002\n",
            "Epoch 32/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0836 - val_loss: 0.0985\n",
            "Epoch 33/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0857 - val_loss: 0.0988\n",
            "Epoch 34/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0816 - val_loss: 0.1023\n",
            "Epoch 35/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0801 - val_loss: 0.0995\n",
            "Epoch 36/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0803 - val_loss: 0.1008\n",
            "Epoch 37/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0842 - val_loss: 0.1000\n",
            "Epoch 38/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0809 - val_loss: 0.1012\n",
            "Epoch 39/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0779 - val_loss: 0.0982\n",
            "Epoch 40/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0853 - val_loss: 0.0997\n",
            "Epoch 41/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0850 - val_loss: 0.0975\n",
            "Epoch 42/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0805 - val_loss: 0.0979\n",
            "Epoch 43/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0773 - val_loss: 0.0987\n",
            "Epoch 44/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0828 - val_loss: 0.0999\n",
            "Epoch 45/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0873 - val_loss: 0.1042\n",
            "Epoch 46/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0851 - val_loss: 0.0987\n",
            "Epoch 47/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0806 - val_loss: 0.0972\n",
            "Epoch 48/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0839 - val_loss: 0.1015\n",
            "Epoch 49/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0880 - val_loss: 0.0988\n",
            "Epoch 50/50\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0790 - val_loss: 0.0994\n"
          ]
        }
      ]
    }
  ]
}